# ============================================================
# Experiment Configuration: Qwen3-32B Suffix Classification
# ============================================================

# General Settings
exp_type: "suffix-task-classification"
exp_name: "qwen3-32B-v1-seed2982"  # 实验名称（会自动成为输出目录名）
seed: 2982

# Training Phase
n_folds: 5 
test_fold: 4      # 使用第 4 折作为验证集（0-indexed）
max_epochs: 1

# System
num_workers: 4    # DataLoader 并行加载进程数

# Logging & Evaluation 
log_interval: 10   # 每 10 个 batch 记录一次日志
eval_interval: 1   # 每 1 个 epoch 评估一次
clip_grad: false   # 是否使用梯度裁剪
cast_to_bf16: false  # 是否强制转换模型参数为 BF16

# Batching
train_batch_size: 32 
eval_batch_size: 16
gradient_accumulation_steps: 1  # 梯度累积步数（1 表示不累积）

# Dataset Configuration
dataset_config:
  csv_file: "/root/autodl-tmp/MAP/data/tascj/dtrainval.csv"
  block_size: 128
  prompt_name: "xxxxx"  # 提示词模板名称

# LLM Configuration
llm_config:
  backbone: "Qwen/Qwen3-32B"  # HuggingFace 模型路径
  model_type: "qwen3" 
  num_labels: 1
  dtype: "bfloat16"
  gradient_checkpointing: true  # 启用梯度检查点节省显存
  resume_from_checkpoint: null  # 如需恢复训练，填写 checkpoint 路径

# Learning Rate Scheduler
lr_scheduler_config: 
  name: "linear_warmup_decay"
  params: 
    warmup_ratio: 0.1       # Warmup 阶段占总步数的 10%
    start_factor: 0.001     # Warmup 起始 LR 倍率
    end_factor: 0.001       # Decay 结束 LR 倍率

# Optimizer
optimizer_config: 
  name: "OffloadAdam"       # 使用 CPU Offload 的 Adam 优化器
  params: 
    lr: 1.0e-5
    decoupled_weight_decay: true
    mode: "stochastic_rounding"  # 随机舍入模式
    verbose: 10             # 每 10 步打印一次优化器信息